# -*- coding: utf-8 -*-
"""FinalProject_Kaggle.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18kNhrIINxPmKuRtX6QMmn42qo9yPUhI5

# CI Final Project(for kaggle) Alireza Rashidi

## Loading Dataset:
"""

from google.colab import drive
drive.mount('/content/drive')

!cp /content/drive/MyDrive/Datasets/train2.zip /content/sample_data
!cp /content/drive/MyDrive/Datasets/CI_test.csv.zip /content/sample_data

!mkdir /content/Final_Project
!unzip /content/sample_data/train2.zip -d /content/Final_Project
!unzip /content/sample_data/CI_test.csv.zip -d /content/Final_Project

import pandas as pd
import numpy as np
from tensorflow import keras
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from tensorflow.keras.layers import SpatialDropout1D
from tensorflow.keras.layers import Embedding
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn import svm
from sklearn.metrics import classification_report, confusion_matrix

df= pd.read_csv('/content/Final_Project/train.csv', sep=',')
df.head(5)

df=df.replace(['\n'],regex=True)

"""## Data Preprocessing(For RNN model):"""

print(df.shape)
# df = df.dropna()
# print(df.shape)

data = df[['comment','recommend']]
data.head(10)

data = data.dropna()
print(df.isnull().values.any())
print(data.shape)

# mapping labels to integer values
sentiment_label = data.recommend.factorize()
sentiment_label

# tokenize words in our dataset
pad_size = 300

data_text = data.comment.values
tokenizer = Tokenizer(num_words=6000)
tokenizer.fit_on_texts(data_text)

vocab_size = len(tokenizer.word_index) + 1
encoded_docs = tokenizer.texts_to_sequences(data_text)

#The sentences have different number of words, therefore, the length of the sequence of numbers will be different. Our model requires inputs to have equal lengths, so we will have to pad the sequence to have the chosen length of inputs
padded_sequence = pad_sequences(encoded_docs, maxlen=pad_size)

print('word mappings: ', tokenizer.word_index)
print('\npadded words: \n', padded_sequence)

idx = 0
print('\nactual text: ', data_text[idx])
print('encodded text: ', encoded_docs[idx])

"""### train_test_validation split:"""

X_train, X_test, Y_train, Y_test = train_test_split(padded_sequence, sentiment_label[0], test_size = 0.10, random_state = 42)
print("train set: ", X_train.shape, Y_train.shape)
print("test set: ", X_test.shape, Y_test.shape)

# validation_len = int(X_test.shape[0] / 2)
# X_val = X_test[:validation_len]
# Y_val = Y_test[:validation_len]

# X_test = X_test[validation_len:]
# Y_test = Y_test[validation_len:]

# print("validation set: ", X_val.shape, Y_val.shape)
# print("test set: ", X_test.shape, Y_test.shape)

# print('\n', X_val[:5])
# print(Y_val[:5], '\n')
# print(X_test[:5])
# print(Y_test[:5])

"""## Creating RNN Model:"""

embedding_vector_length = 32
batch_size = 64

model = Sequential()
model.add(Embedding(vocab_size, embedding_vector_length, input_length=pad_size))
model.add(SpatialDropout1D(0.25))
model.add(LSTM(64, dropout=0.5, recurrent_dropout=0.5))
model.add(Dropout(0.2))

model.add(Dense(1, activation='sigmoid'))   # we have binary classification
model.compile(loss='binary_crossentropy',optimizer='adam', 
                           metrics=['accuracy'])
print(model.summary())

history = model.fit(X_train, Y_train, epochs=12, batch_size=batch_size, validation_data=(X_test, Y_test))

def learningCurve_plot(hs3, epochs3, loss, model_name):    # ploting learining curve plot for created model.
    if loss:
        plt.plot(np.arange(1, epochs3+1), hs3.history["loss"], label="train_loss")
        plt.plot(np.arange(1, epochs3+1), hs3.history["val_loss"], label="val_loss")
        plt.title(" ".join(["Learning curve loss", model_name]))
        plt.xlabel("Epoch #")
        plt.ylabel("Loss")
        plt.legend()
        plt.show()
    else:   
        plt.plot(np.arange(1, epochs3+1), hs3.history["accuracy"], label="train_acc")
        plt.plot(np.arange(1, epochs3+1), hs3.history["val_accuracy"], label="val_acc")
        plt.title(" ".join(["Learning curve accuracy", model_name]))
        plt.xlabel("Epoch #")
        plt.ylabel("Accuracy")
        plt.legend()
        plt.show()

learningCurve_plot(history, 12, True, "LSTM loss")

learningCurve_plot(history, 12, False, "LSTM acc")

"""## Prediction using test set:"""

scores = model.evaluate(X_test, Y_test, verbose=1)
print('Test accuracy:', scores[1])
print('Test loss:', scores[0])

"""### here we test some text input for verifying our models functionalty:"""

# using some test word for prediction...

test_word = 'خوب هست'
tw = tokenizer.texts_to_sequences([test_word])
tw = pad_sequences(tw, maxlen=pad_size)

prediction = int(model.predict(tw).round().item())
print("predicted result is: ", sentiment_label[1][prediction])

"""## Using test data:"""

df_test = pd.read_csv('/content/Final_Project/CI_test.csv', sep=',')
df_test.head(10)

print(df_test['comment'].isnull().sum())
df_test['comment'] = df_test['comment'].fillna('خوب', inplace=False)
print(df_test['comment'].shape)

df_test['comment']

data_test = df_test[['comment']]
# data_test = data_test.dropna()

data_text = data_test.comment.values
tw = tokenizer.texts_to_sequences(data_text)
tw = pad_sequences(tw, maxlen=pad_size)
tw

prediction_results = model.predict(tw)
prediction_results

prediction_results = np.round(prediction_results).astype(int)
prediction_results_labels = sentiment_label[1][prediction_results]
ids = np.array(list(range(1, prediction_results_labels.shape[0]+1))).reshape(-1,1)

mixed = np.concatenate((ids, prediction_results_labels), axis=1)
column_values = ['id', 'recommend']
pd.DataFrame(data = mixed, columns = column_values).to_csv('test_result.csv', header=True, index=False)

"""## Saving model and it's parammeters:"""

model.save('my_model.h5')  # creates a HDF5 file 'my_model.h5'
model.save_weights('my_model_weights.h5', overwrite=True)

"""## Loading model:"""

# model = load_model('my_model.h5')